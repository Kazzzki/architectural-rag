# retriever.py v3 â€” ã‚¯ã‚¨ãƒªå±•é–‹ãƒ»HyDEãƒ»Geminiãƒªãƒ©ãƒ³ã‚¯å¯¾å¿œ
#
# å¤‰æ›´å±¥æ­´:
#   v3 (2026-02-25): ã‚¯ã‚¨ãƒªæ„å›³åˆ†é¡ãƒ»ã‚¯ã‚¨ãƒªå±•é–‹ãƒ»HyDEãƒ»ä¸¦åˆ—æ¤œç´¢ãƒ»Geminiãƒªãƒ©ãƒ³ã‚¯è¿½åŠ 
#                    parent_chunk_id ã‹ã‚‰ã®è¦ªãƒãƒ£ãƒ³ã‚¯å–å¾—ã«å¯¾å¿œ

import os
import json
import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter

logger = logging.getLogger(__name__)

import chromadb

from config import (
    CHROMA_DB_DIR,
    FILE_INDEX_PATH,
    TOP_K_RESULTS,
    COLLECTION_NAME,
)
from indexer import GeminiEmbeddingFunction, get_query_embedding, get_chroma_client, load_parent_chunk
from gemini_client import get_client
from utils.retry import sync_retry
from google.genai import types


# â”€â”€â”€ Collection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_collection():
    """ChromaDB ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    client = get_chroma_client()
    embedding_function = GeminiEmbeddingFunction()
    return client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_function,
    )


# â”€â”€â”€ ã‚¯ã‚¨ãƒªæ„å›³åˆ†é¡ + ã‚¯ã‚¨ãƒªå±•é–‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_INTENT_SYSTEM = """ã‚ãªãŸã¯å»ºç¯‰ RAG ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¯ã‚¨ãƒªã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åˆ†æã—ã€ä»¥ä¸‹ã® JSON ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆæ—¥æœ¬èªã®ã¿ã€Markdown ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ä¸è¦ï¼‰ï¼š

{
  "doc_type_filter": "law" | "drawing" | "spec" | "catalog" | null,
  "expanded_queries": ["æ‹¡å¼µã‚¯ã‚¨ãƒª1", "æ‹¡å¼µã‚¯ã‚¨ãƒª2", ...],
  "hypothetical_doc": "ã“ã®è³ªå•ã«ç­”ãˆã‚‹å»ºç¯‰æŠ€è¡“æ–‡æ›¸ã®ä¸€ç¯€ï¼ˆ300æ–‡å­—ä»¥å†…ï¼‰"
}

åˆ†é¡ãƒ«ãƒ¼ãƒ«:
- æ³•è¦ãƒ»æ¡ä¾‹ãƒ»åŸºæº–ãƒ»å‘Šç¤º â†’ "law"
- å›³é¢ãƒ»ç´ã¾ã‚Šãƒ»è©³ç´°ãƒ»å¹³é¢å›³ãƒ»æ–­é¢å›³ â†’ "drawing"
- ä»•æ§˜ãƒ»å·¥æ³•ãƒ»æ–½å·¥ãƒ»JASSãƒ»JIS â†’ "spec"
- ã‚«ã‚¿ãƒ­ã‚°ãƒ»è£½å“ãƒ»ãƒ¡ãƒ¼ã‚«ãƒ¼ãƒ»ä¾¡æ ¼ â†’ "catalog"
- åˆ¤å®šã§ããªã„ â†’ null

expanded_queries ã¯å»ºç¯‰å°‚é–€ç”¨èªã§ 3ã€œ5 ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆã€‚
hypothetical_doc ã¯ HyDE æ¤œç´¢ç”¨ã«ã€Œå®Ÿéš›ã®æ–‡æ›¸ã®ä¸€ç¯€ã€ã¨ã—ã¦æ›¸ãã“ã¨ã€‚
"""


@sync_retry(max_retries=2, base_wait=1.0)
def _call_gemini_json(prompt: str) -> Dict[str, Any]:
    """Gemini ã§ã‚¯ã‚¨ãƒªæ„å›³åˆ†æã‚’å®Ÿè¡Œã— JSON ã‚’è¿”ã™"""
    client = get_client()
    from config import GEMINI_MODEL_RAG
    response = client.models.generate_content(
        model=GEMINI_MODEL_RAG,
        contents=[
            types.Content(role="user", parts=[types.Part.from_text(text=prompt)])
        ],
        config=types.GenerateContentConfig(
            system_instruction=_INTENT_SYSTEM,
            temperature=0.1,
            max_output_tokens=1024,
        ),
    )
    text = response.text.strip()
    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–ã‚Šé™¤ã
    if text.startswith("```"):
        text = "\n".join(text.split("\n")[1:])
        text = text.rsplit("```", 1)[0]
    return json.loads(text)


def classify_and_expand(query: str) -> Tuple[Optional[str], List[str], str]:
    """
    ã‚¯ã‚¨ãƒªã‚’åˆ†æã—ã¦ (doc_type_filter, expanded_queries, hypothetical_doc) ã‚’è¿”ã™ã€‚
    Gemini å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™ã€‚
    """
    try:
        result = _call_gemini_json(query)
        doc_type_filter = result.get("doc_type_filter")
        expanded = result.get("expanded_queries", [query])
        hypo_doc  = result.get("hypothetical_doc", "")
        return doc_type_filter, expanded, hypo_doc
    except Exception as e:
        logger.warning(f"classify_and_expand failed, using fallback: {e}")
        return None, [query], ""


# â”€â”€â”€ å˜ä¸€ã‚¯ã‚¨ãƒªæ¤œç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _search_single(
    query_text: str,
    collection,
    n: int = TOP_K_RESULTS,
    where: Optional[Dict] = None,
    use_hyde_embedding: bool = False,
) -> List[Dict[str, Any]]:
    """å˜ä¸€ã‚¯ã‚¨ãƒªã§ãƒ•ã‚£ãƒ«ã‚¿ä»˜ããƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€ãƒ’ãƒƒãƒˆä¸€è¦§ã‚’è¿”ã™"""
    if collection.count() == 0:
        return []

    query_embedding = get_query_embedding(query_text)

    kwargs = {
        "query_embeddings": [query_embedding],
        "n_results": n,
        "include": ["documents", "metadatas", "distances"],
    }
    if where:
        kwargs["where"] = where

    try:
        results = collection.query(**kwargs)
    except Exception as e:
        logger.warning(f"ChromaDB query failed ({query_text[:40]}â€¦): {e}")
        return []

    hits = []
    docs = results.get("documents", [[]])[0]
    metas = results.get("metadatas", [[]])[0]
    dists = results.get("distances", [[]])[0]

    for doc, meta, dist in zip(docs, metas, dists):
        hits.append({
            "document":  doc,
            "metadata":  meta,
            "distance":  dist,
            "score":     1.0 - dist,  # ã‚³ã‚µã‚¤ãƒ³è·é›¢ã‚’é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã«å¤‰æ›
        })
    return hits


def _merge_hits(hits_list: List[List[Dict[str, Any]]], top_k: int = 15) -> List[Dict[str, Any]]:
    """è¤‡æ•°æ¤œç´¢çµæœã‚’ãƒãƒ¼ã‚¸ã—ã€ã‚¹ã‚³ã‚¢ãŒé«˜ã„ä¸Šä½ top_k ä»¶ã‚’è¿”ã™ï¼ˆé‡è¤‡é™¤å»ï¼‰"""
    dedup: Dict[str, Dict[str, Any]] = {}
    for hits in hits_list:
        for hit in hits:
            # rel_path + chunk_index ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆparent_chunk_id ã§ã‚‚å¯ï¼‰
            key = (hit["metadata"].get("rel_path", ""), hit["metadata"].get("chunk_index", 0))
            key_str = f"{key[0]}::{key[1]}"
            if key_str not in dedup or hit["score"] > dedup[key_str]["score"]:
                dedup[key_str] = hit
    sorted_hits = sorted(dedup.values(), key=lambda x: x["score"], reverse=True)
    return sorted_hits[:top_k]


# â”€â”€â”€ Gemini ãƒªãƒ©ãƒ³ã‚¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_RERANK_PROMPT = """ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ã§ã™ã‹ï¼Ÿ
0.0ï¼ˆå…¨ãé–¢ä¿‚ãªã„ï¼‰ã€œ1.0ï¼ˆå®Œå…¨ã«é–¢é€£ï¼‰ã®æ•°å€¤ã®ã¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: {query}

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}
"""

@sync_retry(max_retries=2, base_wait=1.0)
def _rerank_single(query: str, context: str) -> float:
    client = get_client()
    from config import GEMINI_MODEL_RAG
    response = client.models.generate_content(
        model=GEMINI_MODEL_RAG,
        contents=[_RERANK_PROMPT.format(query=query, context=context[:500])],
        config=types.GenerateContentConfig(temperature=0.0, max_output_tokens=8),
    )
    try:
        return float(response.text.strip())
    except ValueError:
        return 0.5


def rerank_hits(query: str, hits: List[Dict[str, Any]], threshold: float = 0.5) -> List[Dict[str, Any]]:
    """Gemini ã§ãƒªãƒ©ãƒ³ã‚¯ã—ã€threshold ä»¥ä¸Šã®ã‚‚ã®ä¸Šä½5ä»¶ã‚’è¿”ã™"""
    scored = []
    for hit in hits:
        try:
            score = _rerank_single(query, hit["document"])
        except Exception as e:
            logger.warning(f"rerank_single failed: {e}")
            score = 0.5
        if score >= threshold:
            hit = dict(hit)
            hit["rerank_score"] = score
            scored.append(hit)
    scored.sort(key=lambda x: x["rerank_score"], reverse=True)
    return scored[:5]


# â”€â”€â”€ è¦ªãƒãƒ£ãƒ³ã‚¯å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _resolve_parent_chunks(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å„ãƒãƒ£ãƒ³ã‚¯ã® parent_chunk_id ã‹ã‚‰è¦ªãƒãƒ£ãƒ³ã‚¯ï¼ˆ500ã€œ800æ–‡å­—ï¼‰ã‚’å–å¾—ã—ã€
    LLM å…¥åŠ›ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç½®ãæ›ãˆã‚‹ã€‚
    è¦ªãƒãƒ£ãƒ³ã‚¯ãŒå–å¾—ã§ããªã„å ´åˆã¯å…ƒã®å°ãƒãƒ£ãƒ³ã‚¯ã‚’ãã®ã¾ã¾ä½¿ç”¨ã€‚
    """
    resolved = []
    for hit in hits:
        pid = hit["metadata"].get("parent_chunk_id", "")
        parent_text = load_parent_chunk(pid) if pid else None
        hit = dict(hit)
        hit["context_text"] = parent_text if parent_text else hit["document"]
        resolved.append(hit)
    return resolved


# â”€â”€â”€ ãƒ¡ã‚¤ãƒ³æ¤œç´¢é–¢æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search(
    query: str,
    n_results: int = TOP_K_RESULTS,
    # å¾Œæ–¹äº’æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆv2 API ã¨ã®äº’æ›æ€§ï¼‰
    filter_category: Optional[str] = None,
    filter_file_type: Optional[str] = None,
    filter_date_range: Optional[str] = None,
    filter_tags: Optional[List[str]] = None,
    tag_match_mode: str = "any",
    # v3 æ–°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    use_query_expansion: bool = True,
    use_hyde: bool = True,
    use_rerank: bool = True,
) -> Dict[str, Any]:
    """
    v3 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢:
    1. ã‚¯ã‚¨ãƒªæ„å›³åˆ†é¡ + ã‚¯ã‚¨ãƒªå±•é–‹ + HyDE ä»®èª¬æ–‡æ›¸ç”Ÿæˆ
    2. å„å±•é–‹ã‚¯ã‚¨ãƒª + HyDE ã§ ChromaDB ã‚’ä¸¦åˆ—æ¤œç´¢
    3. ã‚¹ã‚³ã‚¢ãƒãƒ¼ã‚¸ï¼ˆä¸Šä½ 15 ä»¶ï¼‰
    4. Gemini ãƒªãƒ©ãƒ³ã‚¯ï¼ˆ0.5 æœªæº€é™¤å¤– â†’ ä¸Šä½ 5 ä»¶ï¼‰
    5. parent_chunk_id ã‹ã‚‰è¦ªãƒãƒ£ãƒ³ã‚¯å–å¾—
    """
    collection = get_collection()

    if collection.count() == 0:
        return {"documents": [], "metadatas": [], "distances": [], "hits": []}

    # â”€â”€â”€ Step 1: ã‚¯ã‚¨ãƒªæ„å›³åˆ†é¡ãƒ»å±•é–‹ãƒ»HyDE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    doc_type_filter = None
    expanded_queries = [query]
    hypo_doc = ""

    if use_query_expansion or use_hyde:
        try:
            doc_type_filter, expanded_queries, hypo_doc = classify_and_expand(query)
        except Exception as e:
            logger.warning(f"Query expansion failed, using base query: {e}")

    # å¾Œæ–¹äº’æ›ãƒ•ã‚£ãƒ«ã‚¿ãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸå ´åˆã¯å±•é–‹ã§å¾—ãŸãƒ•ã‚£ãƒ«ã‚¿ã‚ˆã‚Šå„ªå…ˆ
    effective_doc_type_filter = filter_file_type or doc_type_filter

    # ChromaDB where æ¡ä»¶
    where: Optional[Dict] = None
    where_conditions = []
    if effective_doc_type_filter and effective_doc_type_filter not in ("md", "pdf"):
        where_conditions.append({"doc_type": {"$eq": effective_doc_type_filter}})
    if filter_category:
        where_conditions.append({"category": {"$eq": filter_category}})
    if filter_date_range:
        from datetime import datetime, timedelta
        now = datetime.now()
        delta_map = {"7d": 7, "1m": 30, "3m": 90}
        days = delta_map.get(filter_date_range)
        if days:
            start_date = now - timedelta(days=days)
            where_conditions.append({"modified_at": {"$gte": start_date.isoformat()}})
    if filter_tags:
        tag_conds = [{"tags_str": {"$contains": t}} for t in filter_tags]
        if tag_match_mode == "all":
            where_conditions.extend(tag_conds)
        else:
            where_conditions.append({"$or": tag_conds} if len(tag_conds) > 1 else tag_conds[0])

    if len(where_conditions) == 1:
        where = where_conditions[0]
    elif len(where_conditions) > 1:
        where = {"$and": where_conditions}

    # â”€â”€â”€ Step 2: ä¸¦åˆ—æ¤œç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_hits_lists = []

    # æ‹¡å¼µã‚¯ã‚¨ãƒªã§æ¤œç´¢
    if use_query_expansion:
        for eq in expanded_queries:
            hits = _search_single(eq, collection, n=n_results, where=where)
            all_hits_lists.append(hits)
    else:
        all_hits_lists.append(_search_single(query, collection, n=n_results, where=where))

    # HyDE æ¤œç´¢
    if use_hyde and hypo_doc:
        all_hits_lists.append(_search_single(hypo_doc, collection, n=n_results, where=where))

    # â”€â”€â”€ Step 3: ãƒãƒ¼ã‚¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    merged_hits = _merge_hits(all_hits_lists, top_k=15)

    # â”€â”€â”€ Step 4: Gemini ãƒªãƒ©ãƒ³ã‚¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if use_rerank and merged_hits:
        try:
            final_hits = rerank_hits(query, merged_hits, threshold=0.5)
            if not final_hits:
                # ãƒªãƒ©ãƒ³ã‚¯ã§å…¨ä»¶é™¤å¤–ã•ã‚ŒãŸå ´åˆã¯ãƒªãƒ©ãƒ³ã‚¯ãªã—ä¸Šä½5ä»¶ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                logger.warning("All hits filtered by rerank, using top-5 fallback")
                final_hits = merged_hits[:5]
        except Exception as e:
            logger.warning(f"Reranking failed, using merged hits: {e}")
            final_hits = merged_hits[:5]
    else:
        final_hits = merged_hits[:n_results]

    # â”€â”€â”€ Step 5: è¦ªãƒãƒ£ãƒ³ã‚¯è§£æ±º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final_hits = _resolve_parent_chunks(final_hits)

    # â”€â”€â”€ å¾Œæ–¹äº’æ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ› â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    documents = [h["document"] for h in final_hits]
    metadatas = [h["metadata"] for h in final_hits]
    distances = [h.get("distance", 0.0) for h in final_hits]

    return {
        "documents": documents,
        "metadatas": metadatas,
        "distances": distances,
        "hits": final_hits,  # v3 æ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆcontext_text ã‚’å«ã‚€ï¼‰
        "doc_type_filter_applied": doc_type_filter,
        "expanded_queries": expanded_queries,
    }


# â”€â”€â”€ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_context(search_results: Dict[str, Any]) -> str:
    """
    æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰ã€‚
    v3 ã§ã¯ context_textï¼ˆè¦ªãƒãƒ£ãƒ³ã‚¯ï¼‰ã‚’ä½¿ç”¨ã—ã€å‡ºå…¸ã« source_pdf_name + page_no ã‚’æ˜è¨˜ã€‚
    """
    hits = search_results.get("hits", [])

    if not hits:
        # å¾Œæ–¹äº’æ›: hits ãŒãªã„å ´åˆã¯å¾“æ¥ã® documents / metadatas ã‚’ä½¿ã†
        documents = search_results.get("documents", [])
        metadatas = search_results.get("metadatas", [])
        if not documents:
            return ""
        context_parts = []
        for doc, meta in zip(documents, metadatas):
            source = meta.get("source_pdf_name") or meta.get("filename", "ä¸æ˜")
            page   = meta.get("page_no") or meta.get("page_number", "")
            page_info = f" (p.{page})" if page else ""
            cat = meta.get("category", "")
            context_parts.append(f"=== å‡ºå…¸: {source}{page_info}ï¼ˆ{cat}ï¼‰===\n{doc}")
        return "\n\n".join(context_parts)

    context_parts = []
    for hit in hits:
        meta = hit.get("metadata", {})
        text = hit.get("context_text") or hit.get("document", "")

        source_name = meta.get("source_pdf_name") or meta.get("filename", "ä¸æ˜")
        page_no     = meta.get("page_no") or meta.get("page_number", "")
        doc_type    = meta.get("doc_type", "")
        category    = meta.get("category", "")

        page_info = f" (p.{page_no})" if page_no else ""
        icon = "ğŸ“ " if doc_type == "drawing" else ""

        context_parts.append(
            f"=== {icon}å‡ºå…¸: {source_name}{page_info}ï¼ˆ{category}ï¼‰===\n{text}"
        )

    return "\n\n".join(context_parts)


# â”€â”€â”€ ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_source_files(search_results: Dict[str, Any]) -> List[Dict[str, Any]]:
    """æ¤œç´¢çµæœã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸æƒ…å ±å«ã‚€ï¼‰"""
    metadatas = search_results.get("metadatas", [])
    file_counter: Counter = Counter()
    file_info_map: Dict[str, Dict] = {}
    file_pages_map: Dict[str, set] = {}

    for meta in metadatas:
        rel_path = meta.get("rel_path", "")
        if not rel_path:
            continue
        file_counter[rel_path] += 1

        page_num = meta.get("page_no") or meta.get("page_number")
        if rel_path not in file_pages_map:
            file_pages_map[rel_path] = set()
        if page_num is not None:
            file_pages_map[rel_path].add(int(page_num))

        if rel_path not in file_info_map:
            category = meta.get("category", "")
            doc_type  = meta.get("doc_type", "")
            file_info_map[rel_path] = {
                "filename":        meta.get("filename", "ä¸æ˜"),
                "source_pdf_name": meta.get("source_pdf_name", meta.get("filename", "ä¸æ˜")),
                "source_pdf_hash": meta.get("source_pdf_hash", ""),
                "rel_path":        rel_path,
                "category":        category,
                "doc_type":        doc_type,
                "tags":            meta.get("tags_str", "").split(",") if meta.get("tags_str") else [],
            }

    source_files = []
    for rel_path, count in file_counter.most_common():
        info = file_info_map[rel_path].copy()
        info["relevance_count"] = count
        info["pages"] = sorted(file_pages_map.get(rel_path, []))
        source_files.append(info)

    return source_files


# â”€â”€â”€ DB çµ±è¨ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_db_stats() -> Dict[str, Any]:
    """ChromaDB ã¨ SQLite ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    try:
        collection = get_collection()
        count = collection.count()

        from database import get_session, Document as DbDocument
        from sqlalchemy import func
        session = get_session()
        try:
            file_count = session.query(DbDocument).filter(
                DbDocument.file_hash.isnot(None)
            ).count()
            latest = session.query(func.max(DbDocument.last_indexed_at)).scalar()
            last_updated = latest.isoformat() if latest else "æœªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"
        finally:
            session.close()

        return {
            "chunk_count":  count,
            "file_count":   file_count,
            "last_updated": last_updated,
        }
    except Exception as e:
        logger.error(f"get_db_stats error: {e}", exc_info=True)
        return {
            "chunk_count":  0,
            "file_count":   0,
            "last_updated": "ã‚¨ãƒ©ãƒ¼",
            "error":        str(e),
        }


def _load_file_index() -> Dict[str, Any]:
    """å¾Œæ–¹äº’æ›: DB ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
    from database import get_session, Document as DbDocument
    session = get_session()
    try:
        docs = session.query(DbDocument).filter(DbDocument.file_hash.isnot(None)).all()
        files = {}
        for doc in docs:
            files[doc.file_path] = {
                "hash":        doc.file_hash,
                "chunk_count": doc.chunk_count or 0,
                "indexed_at":  doc.last_indexed_at.isoformat() if doc.last_indexed_at else None,
                "modified_at": doc.updated_at.isoformat() if doc.updated_at else None,
            }
        return files
    finally:
        session.close()
