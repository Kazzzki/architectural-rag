# generator.py - Gemini APIã§å›ç­”ç”Ÿæˆï¼ˆWebã‚¢ãƒ—ãƒªç‰ˆï¼‰

from typing import List, Dict, Any, AsyncGenerator, Optional

from google.genai import types

from config import GEMINI_MODEL_RAG, MAX_TOKENS, TEMPERATURE
from gemini_client import get_client
from utils.retry import sync_retry
import logging

logger = logging.getLogger(__name__)

# äº’æ›æ€§ã®ãŸã‚
GEMINI_MODEL = GEMINI_MODEL_RAG


SYSTEM_PROMPT = """ã‚ãªãŸã¯å»ºç¯‰æ„åŒ è¨­è¨ˆã®æŠ€è¡“ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
å»ºè¨­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®PM/CMï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ/ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼‰ã®ç«‹å ´ã‹ã‚‰ã€è¨­è¨ˆè€…ã¨æŠ€è¡“çš„ãªè­°è«–ãŒã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€å›ç­”ãƒ«ãƒ¼ãƒ«ã€‘
1. æŠ€è¡“çš„æ ¹æ‹ ã‚’æ˜ç¤ºã™ã‚‹ï¼ˆæ³•ä»¤åãƒ»åŸºæº–åãƒ»ä»•æ§˜æ›¸åã‚’å…·ä½“çš„ã«ï¼‰
2. ã‚³ã‚¹ãƒˆãƒ»å·¥æœŸãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¸ã®å½±éŸ¿ãŒã‚ã‚‹å ´åˆã¯å¿…ãšè¨€åŠã™ã‚‹
3. è¤‡æ•°ã®é¸æŠè‚¢ãŒã‚ã‚‹å ´åˆã¯æ¯”è¼ƒè¡¨å½¢å¼ã§æ•´ç†ã™ã‚‹
4. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã§å›ç­”ã§ããªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ­£ç›´ã«ä¼ãˆã‚‹
5. å›ç­”ã®æœ€å¾Œã«ã€ŒğŸ“ é–¢é€£è³‡æ–™ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¿…ãšè¨­ã‘ã‚‹
6. æ—¥æœ¬ã®å»ºç¯‰åŸºæº–æ³•ãƒ»JISãƒ»JASSç­‰ã®æ—¥æœ¬å›½å†…åŸºæº–ã«åŸºã¥ã
7. å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã†ã“ã¨
8. å‡ºå…¸ã«ã¯å¿…ãšã€Œãƒ•ã‚¡ã‚¤ãƒ«å (p.XX)ã€ã®å½¢å¼ã§ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æ˜è¨˜ã™ã‚‹
9. å›³é¢ï¼ˆdoc_type=drawingï¼‰ã‹ã‚‰ã®å‡ºå…¸ã«ã¯ã€ŒğŸ“ã€ã‚¢ã‚¤ã‚³ãƒ³ã‚’ä»˜ä¸ã™ã‚‹

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
å›ç­”æœ¬æ–‡
ï¼ˆMarkdownå½¢å¼ã€è¦‹å‡ºã—ãƒ»ç®‡æ¡æ›¸ããƒ»è¡¨ã‚’é©å®œä½¿ç”¨ï¼‰

ğŸ“ é–¢é€£è³‡æ–™:
- [ãƒ•ã‚¡ã‚¤ãƒ«å]ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰p.XX
- ğŸ“ [å›³é¢ãƒ•ã‚¡ã‚¤ãƒ«å]ï¼ˆå›³é¢ï¼‰p.XX
"""

# ãƒ•ãƒ­ãƒ³ãƒˆã‹ã‚‰å—ã‘å–ã‚‹ä¼šè©±å±¥æ­´ã®1ä»¶ã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°ï¼ˆé•·ã„å›ç­”ã‚’åˆ‡ã‚Šè©°ã‚ã¦ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
_HISTORY_MAX_CONTENT_CHARS = 2000


def _build_contents(
    user_prompt: str,
    history: Optional[List[Dict]] = None
) -> List[types.Content]:
    """
    ä¼šè©±å±¥æ­´ï¼ˆhistoryï¼‰ã¨ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ Gemini ã® Contents ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    history ã® role ã¯ "user" / "assistant" ã‚’æƒ³å®šã€‚Gemini ã§ã¯ "model" ã«å¤‰æ›ã™ã‚‹ã€‚
    ç›´è¿‘ 10 ä»¶ï¼ˆ5å¾€å¾©ï¼‰ã®ã¿ä½¿ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³è¶…éã‚’é˜²ãã€‚
    """
    contents: List[types.Content] = []

    if history:
        # ç›´è¿‘10ä»¶ã«åˆ¶é™ï¼ˆå¤ã„ã»ã©çœç•¥ï¼‰
        recent = history[-10:]
        for msg in recent:
            role = "user" if msg.get("role") == "user" else "model"
            content = (msg.get("content") or "").strip()
            if not content:
                continue
            # é•·ã„å›ç­”ã¯åˆ‡ã‚Šè©°ã‚
            if len(content) > _HISTORY_MAX_CONTENT_CHARS:
                content = content[:_HISTORY_MAX_CONTENT_CHARS] + "â€¦ï¼ˆçœç•¥ï¼‰"
            contents.append(types.Content(
                role=role,
                parts=[types.Part.from_text(text=content)]
            ))

    # ç¾åœ¨ã®è³ªå•ã‚’æœ«å°¾ã«è¿½åŠ 
    contents.append(types.Content(
        role="user",
        parts=[types.Part.from_text(text=user_prompt)]
    ))
    return contents


@sync_retry(max_retries=3, base_wait=2.0)
def _call_gemini_generate(client, model, contents, config):
    return client.models.generate_content(
        model=model,
        contents=contents,
        config=config
    )

def generate_answer(
    question: str,
    context: str,
    source_files: List[Dict[str, Any]],
    history: Optional[List[Dict]] = None,
) -> str:
    """Gemini APIã§å›ç­”ã‚’ç”Ÿæˆï¼ˆä¼šè©±å±¥æ­´å¯¾å¿œï¼‰"""

    source_files_formatted = "\n".join([
        f"- {file['filename']}ï¼ˆ{file['category']}ï¼‰"
        for file in source_files
    ])

    if not context.strip():
        context = "ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æ¤œç´¢çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰"

    user_prompt = f"""ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢ã•ã‚ŒãŸæƒ…å ±ã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€åˆ©ç”¨å¯èƒ½ãªé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå›ç­”æœ«å°¾ã®ã€ŒğŸ“ é–¢é€£è³‡æ–™ã€ã«å«ã‚ã‚‹ã“ã¨ï¼‰ã€‘
{source_files_formatted if source_files_formatted.strip() else "ï¼ˆé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰"}
"""

    try:
        client = get_client()
        contents = _build_contents(user_prompt, history)
        config = types.GenerateContentConfig(
            system_instruction=SYSTEM_PROMPT,
            temperature=TEMPERATURE,
            max_output_tokens=MAX_TOKENS,
        )
        response = _call_gemini_generate(client, GEMINI_MODEL, contents, config)
        return response.text

    except Exception as e:
        logger.error(f"Gemini generation failed: {e}", exc_info=True)
        raise RuntimeError("AIå›ç­”ç”Ÿæˆã«ä¸€æ™‚çš„ãªå•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")


@sync_retry(max_retries=3, base_wait=2.0)
def _call_gemini_stream(client, model, contents, config):
    # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®åˆæœŸåŒ–è‡ªä½“ã‚’ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã«ã™ã‚‹
    return client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=config
    )

from typing import List, Dict, Any, Optional, Iterator

def generate_answer_stream(
    question: str,
    context: str,
    source_files: List[Dict[str, Any]],
    history: Optional[List[Dict]] = None,
) -> Iterator[str]:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§å›ç­”ã‚’ç”Ÿæˆï¼ˆä¼šè©±å±¥æ­´å¯¾å¿œï¼‰"""

    source_files_formatted = "\n".join([
        f"- {file['filename']}ï¼ˆ{file['category']}ï¼‰"
        for file in source_files
    ])

    if not context.strip():
        context = "ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æ¤œç´¢çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰"

    user_prompt = f"""ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢ã•ã‚ŒãŸæƒ…å ±ã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€åˆ©ç”¨å¯èƒ½ãªé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå›ç­”æœ«å°¾ã®ã€ŒğŸ“ é–¢é€£è³‡æ–™ã€ã«å«ã‚ã‚‹ã“ã¨ï¼‰ã€‘
{source_files_formatted if source_files_formatted.strip() else "ï¼ˆé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰"}
"""

    client = get_client()
    contents = _build_contents(user_prompt, history)
    config = types.GenerateContentConfig(
        system_instruction=SYSTEM_PROMPT,
        temperature=TEMPERATURE,
        max_output_tokens=MAX_TOKENS,
    )
    
    stream_iter = _call_gemini_stream(client, GEMINI_MODEL, contents, config)
    for chunk in stream_iter:
        if chunk.text:
            yield chunk.text
