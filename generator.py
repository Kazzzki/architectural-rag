# generator.py - Gemini 3.0 Flash APIã§å›ç­”ç”Ÿæˆï¼ˆWebã‚¢ãƒ—ãƒªç‰ˆï¼‰

from typing import List, Dict, Any, AsyncGenerator

import google.generativeai as genai

from config import GEMINI_MODEL, MAX_TOKENS, TEMPERATURE, GEMINI_API_KEY

# Gemini APIè¨­å®š
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)


SYSTEM_PROMPT = """ã‚ãªãŸã¯å»ºç¯‰æ„åŒ è¨­è¨ˆã®æŠ€è¡“ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
å»ºè¨­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®PM/CMï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ/ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼‰ã®ç«‹å ´ã‹ã‚‰ã€è¨­è¨ˆè€…ã¨æŠ€è¡“çš„ãªè­°è«–ãŒã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€å›ç­”ãƒ«ãƒ¼ãƒ«ã€‘
1. æŠ€è¡“çš„æ ¹æ‹ ã‚’æ˜ç¤ºã™ã‚‹ï¼ˆæ³•ä»¤åãƒ»åŸºæº–åãƒ»ä»•æ§˜æ›¸åã‚’å…·ä½“çš„ã«ï¼‰
2. ã‚³ã‚¹ãƒˆãƒ»å·¥æœŸãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¸ã®å½±éŸ¿ãŒã‚ã‚‹å ´åˆã¯å¿…ãšè¨€åŠã™ã‚‹
3. è¤‡æ•°ã®é¸æŠè‚¢ãŒã‚ã‚‹å ´åˆã¯æ¯”è¼ƒè¡¨å½¢å¼ã§æ•´ç†ã™ã‚‹
4. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã§å›ç­”ã§ããªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ­£ç›´ã«ä¼ãˆã‚‹
5. å›ç­”ã®æœ€å¾Œã«ã€ŒğŸ“ é–¢é€£è³‡æ–™ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¿…ãšè¨­ã‘ã‚‹
6. æ—¥æœ¬ã®å»ºç¯‰åŸºæº–æ³•ãƒ»JISãƒ»JASSç­‰ã®æ—¥æœ¬å›½å†…åŸºæº–ã«åŸºã¥ã

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
å›ç­”æœ¬æ–‡
ï¼ˆMarkdownå½¢å¼ã€è¦‹å‡ºã—ãƒ»ç®‡æ¡æ›¸ããƒ»è¡¨ã‚’é©å®œä½¿ç”¨ï¼‰

ğŸ“ é–¢é€£è³‡æ–™:
- [ãƒ•ã‚¡ã‚¤ãƒ«å]ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰
"""


def generate_answer(
    question: str,
    context: str,
    source_files: List[Dict[str, Any]]
) -> str:
    """Gemini 3.0 Flash APIã§å›ç­”ã‚’ç”Ÿæˆ"""
    
    source_files_formatted = "\n".join([
        f"- {file['filename']}ï¼ˆ{file['category']}ï¼‰"
        for file in source_files
    ])
    
    if not context.strip():
        context = "ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æ¤œç´¢çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰"
    
    user_prompt = f"""ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢ã•ã‚ŒãŸæƒ…å ±ã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€åˆ©ç”¨å¯èƒ½ãªé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå›ç­”æœ«å°¾ã®ã€ŒğŸ“ é–¢é€£è³‡æ–™ã€ã«å«ã‚ã‚‹ã“ã¨ï¼‰ã€‘
{source_files_formatted if source_files_formatted.strip() else "ï¼ˆé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰"}
"""
    
    try:
        model = genai.GenerativeModel(
            model_name=GEMINI_MODEL,
            generation_config={
                "temperature": TEMPERATURE,
                "max_output_tokens": MAX_TOKENS,
            },
            system_instruction=SYSTEM_PROMPT
        )
        
        response = model.generate_content(user_prompt)
        return response.text
        
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n\nã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"


async def generate_answer_stream(
    question: str,
    context: str,
    source_files: List[Dict[str, Any]]
) -> AsyncGenerator[str, None]:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§å›ç­”ã‚’ç”Ÿæˆ"""
    
    source_files_formatted = "\n".join([
        f"- {file['filename']}ï¼ˆ{file['category']}ï¼‰"
        for file in source_files
    ])
    
    if not context.strip():
        context = "ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æ¤œç´¢çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰"
    
    user_prompt = f"""ä»¥ä¸‹ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢ã•ã‚ŒãŸæƒ…å ±ã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€åˆ©ç”¨å¯èƒ½ãªé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå›ç­”æœ«å°¾ã®ã€ŒğŸ“ é–¢é€£è³‡æ–™ã€ã«å«ã‚ã‚‹ã“ã¨ï¼‰ã€‘
{source_files_formatted if source_files_formatted.strip() else "ï¼ˆé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰"}
"""
    
    try:
        model = genai.GenerativeModel(
            model_name=GEMINI_MODEL,
            generation_config={
                "temperature": TEMPERATURE,
                "max_output_tokens": MAX_TOKENS,
            },
            system_instruction=SYSTEM_PROMPT
        )
        
        response = model.generate_content(user_prompt, stream=True)
        
        for chunk in response:
            if chunk.text:
                yield chunk.text
                
    except Exception as e:
        yield f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
